{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpAuMVCwfWs8"
      },
      "source": [
        "# Final Project : Stock Price Prediction (Google)\n",
        "##Mohammad Jahed Murad Sunny\n",
        "# T00707169\n",
        "Artificial Intelligence - CPSC 7373"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxJfRe4bfYVA"
      },
      "source": [
        "## Part 1 - Data Preprocessing\n",
        "Data preprocessing refers to the process of preparing raw data for machine learning models. The purpose of this step is to clean, transform, and prepare data in a suitable format that can be used by machine learning algorithms.\n",
        "\n",
        "Cleaning the data involves identifying and removing irrelevant, incomplete, or inaccurate data points. For example, if some data points are missing or contain incorrect information, they can be removed or replaced with suitable values to ensure that the data is consistent and complete.\n",
        "\n",
        "Data transformation involves converting the data into a suitable format for analysis. This step can include scaling or normalizing the data to ensure that all features are on the same scale, encoding categorical variables to enable them to be used in machine learning models, and extracting useful features from the data that can help in making predictions.\n",
        "\n",
        "Data preparation involves splitting the data into training and testing sets to evaluate the performance of the machine learning models. This step can also involve feature selection, where only the most important features are selected to improve the accuracy of the models.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir9zwETrfbrp"
      },
      "source": [
        "### Downloading files in your own google drive\n",
        "The first line imports the os library and the second line imports the drive module from the google.colab library. The drive.mount() function is called with the parameter '/content/drive' to mount the Google Drive folder in Colab.\n",
        "\n",
        "The next block of code checks if the training and testing data files already exist in the current directory. If they do, it prints the message 'File exists'. If they don't exist, it downloads them using the !wget command and the URL of the files.\n",
        "\n",
        "The !wget command is used to download files from the internet in Colab. In this case, it is downloading the files from Google Drive using the file ID provided in the URL. The -O option is used to specify the output file name."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "\n",
        "##Download Train Data\n",
        "if os.path.exists('Google_Stock_Price_Train.csv'):\n",
        "  print('File exists')\n",
        "else:\n",
        "  !wget -O Google_Stock_Price_Train.csv \"https://drive.google.com/uc?export=download&id=1WQO-v_ofXWWQ7gl2B11FUzjl9okW32ul\"\n",
        "\n",
        "##Download Test Data\n",
        "if os.path.exists('Google_Stock_Price_Test.csv'):\n",
        "  print('File exists')\n",
        "else:\n",
        "  !wget -O Google_Stock_Price_Test.csv \"https://drive.google.com/uc?export=download&id=10rAwHmB8pSxbuJbxGERQhgZsNsHkH8Mf\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9wqW7EqBAgG",
        "outputId": "d9ad881c-9f94-45df-ad4c-d63b4693f467"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "File exists\n",
            "File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the libraries\n",
        "NumPy is a library for numerical computing in Python, and it provides tools for working with arrays and matrices.\n",
        "\n",
        "Matplotlib is a library for creating static, animated, and interactive visualizations in Python. It provides a variety of visualization tools to create line charts, scatter plots, bar charts, histograms, and more.\n",
        "\n",
        "Pandas is a library for data manipulation and analysis in Python. It provides data structures and tools for working with structured data, such as data frames and series.\n",
        "\n",
        "By importing these libraries, the code can use the functions and tools provided by these libraries to perform tasks such as data visualization, data manipulation, and numerical computing.\n"
      ],
      "metadata": {
        "id": "isGXBuqIWhmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "jRQi5JrPqd5w"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ47JAxrgmaL"
      },
      "source": [
        "### Importing the training set\n",
        "The first line imports the Pandas library and uses it to read the training data from a CSV file named 'Google_Stock_Price_Train.csv'. The data is stored in a Pandas dataframe named 'dataset_train'.\n",
        "\n",
        "The second line extracts the relevant training data from the 'dataset_train' dataframe. The 'iloc' function is used to select all rows and the second column (index 1) of the dataframe, which contains the opening stock price values. The '.values' function is used to convert this selection into a NumPy array named 'training_set'."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = pd.read_csv('/content/Google_Stock_Price_Train.csv')\n",
        "training_set = dataset_train.iloc[:, 1:2].values"
      ],
      "metadata": {
        "id": "JHmNwLNEu6Dm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT8_2UJegtG5"
      },
      "source": [
        "### Feature Scaling\n",
        "The 'MinMaxScaler' function scales the data by subtracting the minimum value and dividing by the range of the data. By default, it scales the data to the range of (0, 1), which can be adjusted by specifying the 'feature_range' parameter.\n",
        "\n",
        "In this code snippet, the 'MinMaxScaler' function is initialized with the feature range of (0, 1) and stored in the variable 'sc'.\n",
        "\n",
        "The 'fit_transform' method of the 'sc' object is then applied to the 'training_set' array to scale the data. The scaled data is stored in a new variable named 'training_set_scaled'.\n",
        "\n",
        "This code snippet is useful for scaling the data to a specific range, which can be helpful for improving the accuracy of machine learning models. Many machine learning algorithms perform better when the input features are scaled to a similar range, and this can help prevent certain features from dominating others.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " from sklearn.preprocessing import MinMaxScaler\n",
        " sc = MinMaxScaler(feature_range= (0, 1))\n",
        " training_set_scaled = sc.fit_transform(training_set)"
      ],
      "metadata": {
        "id": "kwU8xVQ5wamY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyYgYocqhNUg"
      },
      "source": [
        "### Creating a data structure with 60 timesteps and 1 output\n",
        "This code snippet is used to create a supervised learning dataset from the scaled training data, which can be used to train a machine learning model for the Google stock price prediction project.\n",
        "\n",
        "The first two lines initialize empty lists named 'x_train' and 'y_train', which will be used to store the input and output data for the supervised learning dataset.\n",
        "\n",
        "The 'for' loop then iterates over the range of indices from 60 to 1258 (the length of the training set), with a step size of 1. This range is chosen because the model will use the previous 60 stock prices to predict the next stock price.\n",
        "\n",
        "For each iteration of the loop, the 'x_train' list is appended with a subsequence of 60 consecutive stock prices, starting from the current index 'i-60' and ending at 'i'. The 'y_train' list is appended with the stock price value at index 'i'.\n",
        "\n",
        "Finally, the 'x_train' and 'y_train' lists are converted to NumPy arrays using the 'np.array' function and stored in their respective variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.ma.core import append\n",
        "x_train = []\n",
        "y_train = []\n",
        "for i in range(60, 1258):\n",
        "  x_train.append(training_set_scaled[i-60:i, 0])\n",
        "  y_train.append(training_set_scaled[i, 0])\n",
        "x_train, y_train = np.array(x_train), np.array(y_train)\n"
      ],
      "metadata": {
        "id": "gb0Au1uox59-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8yaN7Zvi95l"
      },
      "source": [
        "### Reshaping\n",
        "\n",
        "The 'x_train' input data was created in the previous code snippet as a 2D array with dimensions (batch_size, time_steps), where 'batch_size' refers to the number of training examples in each batch, and 'time_steps' refers to the length of the input sequence (in this case, 60).\n",
        "\n",
        "However, an RNN expects a 3D input array with dimensions (batch_size, time_steps, input_dim), where 'input_dim' refers to the number of input features at each time step. In this case, there is only one input feature, which is the stock price value.\n",
        "\n",
        "To reshape the 'x_train' array to the correct shape, this code snippet uses the 'np.reshape' function, which takes two arguments: the input array and the new shape. The new shape is specified as (x_train.shape[0], x_train.shape[1], 1), which means that the first two dimensions of the input shape will remain the same, but a new third dimension of size 1 will be added to represent the input feature dimension.\n",
        "\n",
        "After this code snippet is executed, the 'x_train' array will have a 3D shape of (batch_size, time_steps, 1), which can be used as input to an RNN for training the Google stock price prediction model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.reshape(x_train,(x_train.shape[0], x_train.shape[1], 1))"
      ],
      "metadata": {
        "id": "24B5Lq9l3e-N"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRRSOJeVjEWV"
      },
      "source": [
        "## Part 2 - Building and Training the RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4XV88JMjHXG"
      },
      "source": [
        "### Importing the Keras libraries and packages\n",
        "\n",
        "'Sequential' is a class from Keras that allows us to build a model layer by layer.\n",
        "\n",
        "'Dense' is a type of layer in Keras that represents a fully connected neural network layer.\n",
        "\n",
        "'LSTM' is a type of layer in Keras that represents a Long Short-Term Memory (LSTM) unit, which is a type of recurrent neural network (RNN) that is well-suited for sequential data.\n",
        "\n",
        "'Dropout' is a type of layer in Keras that applies dropout regularization to the input, which helps prevent overfitting during training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout"
      ],
      "metadata": {
        "id": "C6nkFkyf6BoU"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEIE-1s9jNzC"
      },
      "source": [
        "### Initialising the RNN\n",
        "\n",
        "A sequential model is a linear stack of layers where each layer is added sequentially. This type of model is appropriate for most deep learning tasks where the data has a clear input and output structure, as is the case with the Google stock price prediction task.\n",
        "\n",
        "The sequential model is stored in the regressor variable and will be used to define the architecture of the LSTM-based RNN model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regressor = Sequential()"
      ],
      "metadata": {
        "id": "LesgOLF-6YE7"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62eg1OPGjT8z"
      },
      "source": [
        "### Adding the first LSTM layer and some Dropout regularisation\n",
        "\n",
        "The add() method is used to add a new layer to the regressor model. In this case, the layer being added is an LSTM layer with 50 memory units (also known as hidden units or cells).\n",
        "\n",
        "The return_sequences argument is set to True in order to return the full sequence of output values, rather than just the last output value, from this layer. This is necessary because the output of this layer will be fed into subsequent LSTM layers.\n",
        "\n",
        "The input_shape argument is set to (x_train.shape[1], 1), which specifies the shape of the input data for this layer. The first dimension of the input shape (x_train.shape[1]) corresponds to the number of time steps in the input sequence, which is 60 in this case. The second dimension of the input shape (1) corresponds to the number of input features at each time step, which is 1 (the stock price value).\n",
        "\n",
        "After the LSTM layer is added, a Dropout layer is added to the model with a dropout rate of 0.2. This helps prevent overfitting during training by randomly dropping out (i.e., setting to zero) some of the output values from the LSTM layer during each training iteration.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (x_train.shape[1], 1)))\n",
        "regressor.add(Dropout(0.2))"
      ],
      "metadata": {
        "id": "Rl0nD5vd6z1S"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XBIYLyOjlMx"
      },
      "source": [
        "### Adding a second LSTM layer and some Dropout regularisation\n",
        "\n",
        "Similar to the first LSTM layer, the add() method is used to add a new layer to the regressor model. In this case, the layer being added is another LSTM layer with 50 memory units.\n",
        "\n",
        "The return_sequences argument is set to True in order to return the full sequence of output values from this layer, which will be fed into the next LSTM layer.\n",
        "\n",
        "After the second LSTM layer is added, another Dropout layer is added to the model with a dropout rate of 0.2. This helps prevent overfitting during training by randomly dropping out some of the output values from the LSTM layer during each training iteration."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.add(LSTM(units = 50, return_sequences = True))\n",
        "regressor.add(Dropout(0.2))"
      ],
      "metadata": {
        "id": "TKjmxzOm8zJZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey3fHVnGj1cu"
      },
      "source": [
        "### Adding a third LSTM layer and some Dropout regularisation\n",
        "Similar to the first two LSTM layers, the add() method is used to add a new layer to the regressor model. In this case, the layer being added is another LSTM layer with 50 memory units.\n",
        "\n",
        "The return_sequences argument is set to True in order to return the full sequence of output values from this layer, which will be fed into the next LSTM layer.\n",
        "\n",
        "After the third LSTM layer is added, another Dropout layer is added to the model with a dropout rate of 0.2. This helps prevent overfitting during training by randomly dropping out some of the output values from the LSTM layer during each training iteration."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.add(LSTM(units = 50, return_sequences = True))\n",
        "regressor.add(Dropout(0.2))"
      ],
      "metadata": {
        "id": "i-u5euNi9akc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYTrtfTmj933"
      },
      "source": [
        "### Adding a fourth LSTM layer and some Dropout regularisation\n",
        "Similar to the previous LSTM layers, the add() method is used to add a new layer to the regressor model. In this case, the layer being added is another LSTM layer with 50 memory units.\n",
        "\n",
        "Since this is the final LSTM layer, the return_sequences argument is not specified and defaults to False. This means that only the last output value from this layer will be returned, rather than the full sequence of output values.\n",
        "\n",
        "After the final LSTM layer is added, another Dropout layer is added to the model with a dropout rate of 0.2. This helps prevent overfitting during training by randomly dropping out some of the output values from the LSTM layer during each training iteration.\n",
        "\n",
        "Note that after the LSTM layers and dropout layers are added to the model, a fully connected Dense layer will be added to the model in the next step."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.add(LSTM(units = 50))\n",
        "regressor.add(Dropout(0.2))"
      ],
      "metadata": {
        "id": "vrMvpvON9b2c"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ABI6rOIkHhk"
      },
      "source": [
        "### Adding the output layer\n",
        "The add() method is used to add a new layer to the regressor model. In this case, the layer being added is a Dense layer with a single output unit.\n",
        "\n",
        "The units argument specifies the number of output units in the Dense layer. Since we want to predict a single output value (the next day's stock price), we set units to 1.\n",
        "\n",
        "Note that since we are using a regression model, rather than a classification model, we do not use an activation function in the output layer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.add(Dense(units = 1))"
      ],
      "metadata": {
        "id": "SBOKGLAl-L5s"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLx4K7uUkPSh"
      },
      "source": [
        "### Compiling the RNN\n",
        "\n",
        "The compile() method is called on the regressor model, and we specify two arguments:\n",
        "\n",
        "optimizer: This argument specifies the optimizer algorithm to use during training. In this case, we specify 'adam' as the optimizer, which is a popular choice for deep learning models.\n",
        "\n",
        "loss: This argument specifies the loss function to use during training. In this case, we specify 'mean_squared_error' as the loss function, which is a commonly used loss function for regression problems.\n",
        "\n",
        "By calling compile() on the regressor model with these arguments, we are setting up the model to be trained using the Adam optimizer algorithm and to minimize the mean squared error loss function during training."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')"
      ],
      "metadata": {
        "id": "8ZC8sLfZ-5iL"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mPhwKGkkebi"
      },
      "source": [
        "### Fitting the RNN to the Training set\n",
        "The fit() method is called on the regressor model, and we specify four arguments:\n",
        "\n",
        "x_train: This argument specifies the input training data, which consists of the scaled stock price values for the previous 60 days.\n",
        "\n",
        "y_train: This argument specifies the output training data, which consists of the scaled stock price value for the current day.\n",
        "\n",
        "epochs: This argument specifies the number of epochs (training iterations) to run during training. In this case, we specify 100 epochs.\n",
        "\n",
        "batch_size: This argument specifies the number of training examples to use in each batch during training. In this case, we specify a batch size of 32.\n",
        "\n",
        "By calling fit() on the regressor model with these arguments, we are training the model using the input and output training data that we have prepared. During training, the model will use the Adam optimizer algorithm to minimize the mean squared error loss function, and it will run for 100 epochs with a batch size of 32. After training is complete, the trained model will be able to predict the stock price for the next day based on the previous 60 days of stock price data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.fit(x_train, y_train, epochs = 100, batch_size = 32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZ3KV7xzAPjT",
        "outputId": "b1126d3b-48bb-4fc2-93d0-66ba513f8e03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "38/38 [==============================] - 27s 176ms/step - loss: 0.0444\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 6s 164ms/step - loss: 0.0060\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 5s 140ms/step - loss: 0.0055\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 8s 203ms/step - loss: 0.0054\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 7s 197ms/step - loss: 0.0055\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 7s 187ms/step - loss: 0.0052\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 6s 149ms/step - loss: 0.0048\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 5s 141ms/step - loss: 0.0049\n",
            "Epoch 9/100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hRau_lIkrE8"
      },
      "source": [
        "## Part 3 - Making the predictions and visualising the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgJO6qEDksxD"
      },
      "source": [
        "### Getting the real stock price of 2017\n",
        "\n",
        "The read_csv() function from the Pandas library is used to read the CSV file and store it as a DataFrame in the dataset_test variable.\n",
        "\n",
        "Then, we use the iloc[] method to extract the values of the second column (i.e., the stock price values) from the dataset_test DataFrame and store them in the real_stock_price variable as a NumPy array.\n",
        "\n",
        "By doing this, we are obtaining the actual stock price values for the test period, which we can use later to evaluate the performance of our trained model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test = pd.read_csv('/content/Google_Stock_Price_Test.csv')\n",
        "real_stock_price = dataset_test.iloc[:, 1:2].values"
      ],
      "metadata": {
        "id": "7kU9X4y3EWcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrvrLblxkz42"
      },
      "source": [
        "### Getting the predicted stock price of 2017\n",
        "\n",
        "First, we concatenate the training and test datasets along the vertical axis using the concat() function from the Pandas library. The resulting dataset_total contains all the historical data for the Google stock price.\n",
        "\n",
        "Next, we extract the values of the last 80 days from dataset_total (i.e., the 60 days preceding the test period and the 20-day test period) and scale them using the same MinMaxScaler instance (sc) that was used to scale the training data.\n",
        "\n",
        "Then, we create a list x_test of input sequences of 60 timesteps for the test period, similar to how we created x_train for the training period.\n",
        "\n",
        "We then reshape x_test into a 3D array with shape (num_samples, 60, 1).\n",
        "\n",
        "Next, we use the trained LSTM model regressor to predict the stock prices for the test period. We store the predicted values in the predicted_stock_price variable.\n",
        "\n",
        "Finally, we use the inverse_transform() method of the sc instance to rescale the predicted stock prices back to their original values. This gives us the final predicted stock prices for the test period."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_total= pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0)\n",
        "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values\n",
        "inputs = inputs.reshape(-1, 1)\n",
        "inputs = sc.transform(inputs)\n",
        "x_test = []\n",
        "for i in range(60, 80):\n",
        "  x_test.append(inputs[i-60:i, 0])\n",
        "x_test = np.array(x_test)\n",
        "x_test = np.reshape(x_test,(x_test.shape[0], x_test.shape[1], 1))\n",
        "predicted_stock_price = regressor.predict(x_test)\n",
        "predicted_stock_price = sc.inverse_transform(predicted_stock_price)"
      ],
      "metadata": {
        "id": "bcoTl3GEFJQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFTNs3YHk6FQ"
      },
      "source": [
        "### Visualising the results\n",
        "\n",
        "First, we plot the real stock prices for the test period in red color using the plot() function. The real_stock_price variable contains the actual stock prices for the test period.\n",
        "\n",
        "Next, we plot the predicted stock prices for the same period in blue color using the plot() function. The predicted_stock_price variable contains the predicted stock prices for the test period.\n",
        "\n",
        "Then, we add a title to the plot using the title() function, with the text \"Google Stock Price Prediction\".\n",
        "\n",
        "We label the x-axis as \"Time\" using the xlabel() function.\n",
        "\n",
        "We label the y-axis as \"Google Stock Price\" using the ylabel() function.\n",
        "\n",
        "Finally, we display a legend for the plot using the legend() function, which shows which line represents the real stock prices and which line represents the predicted stock prices.\n",
        "\n",
        "The show() function is then called to display the plot."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(real_stock_price, color = 'red', label = 'Real Google Stock Price')\n",
        "plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted Google Stock Price')\n",
        "plt.title('Google Stock Price Prediction')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Google Stock Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cZ0lsHFgLmqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Future suggestions for this Google stock price prediction project:\n",
        "\n",
        "Evaluate the model performance: Use various evaluation metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE) to evaluate the performance of the model.\n",
        "\n",
        "Hyperparameter tuning: Experiment with different hyperparameters such as the number of LSTM layers, number of neurons in each layer, number of epochs, and batch size to improve the model performance.\n",
        "\n",
        "Use additional data: Try using additional data such as news articles, economic indicators, and financial statements to improve the accuracy of the model.\n",
        "\n",
        "Test on other stocks: Try using the same model to predict the stock prices of other companies and compare the results to see if the model is generalizable.\n",
        "\n",
        "Use a different model architecture: Try using a different model architecture such as Convolutional Neural Networks (CNN) or a combination of LSTM and CNN to see if it improves the performance of the model.\n",
        "\n",
        "Deploy the model: Deploy the model on a web platform to allow users to access the predictions and interact with the model."
      ],
      "metadata": {
        "id": "K7Y-QqHCAQ30"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sTX4pFhfAUG2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}